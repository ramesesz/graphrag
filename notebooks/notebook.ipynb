{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ce9d6db-5b10-4f18-94b8-dc3e2c1a4b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use /app as base directory if it exists (Docker), otherwise use current directory\n",
    "APP_DIR = Path(\"/app\") if Path(\"/app\").exists() else Path.cwd()\n",
    "DATA_DIR = APP_DIR / \"data\"\n",
    "INPUT_DIR = DATA_DIR / \"input_docs\"\n",
    "OUTPUT_DIR = DATA_DIR / \"output_json\"\n",
    "CHUNKS_DIR = DATA_DIR / \"output_chunks\"\n",
    "OLLAMA_URL = os.environ.get(\"OLLAMA_BASE_URL\", \"http://ollama:11434\")\n",
    "\n",
    "print(f\"[i] App directory: {APP_DIR}\")\n",
    "print(f\"[i] Data directory: {DATA_DIR}\")\n",
    "print(f\"[i] Input directory: {INPUT_DIR}\")\n",
    "print(f\"[i] Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Ensure output directories exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHUNKS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Connect to Neo4j\n",
    "# (Retry loop because Neo4j takes time to initialize the bolt protocol)\n",
    "graph = None\n",
    "max_retries = 30\n",
    "retry_delay = 2\n",
    "for i in range(max_retries):\n",
    "    try:\n",
    "        graph = Neo4jGraph(\n",
    "            url=os.environ[\"NEO4J_URI\"],\n",
    "            username=\"neo4j\",\n",
    "            password=os.environ[\"NEO4J_PASSWORD\"]\n",
    "        )\n",
    "        graph.refresh_schema()\n",
    "        print(\"   [✓] Connected to Neo4j\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        remaining_attempts = max_retries - i - 1\n",
    "        if remaining_attempts > 0:\n",
    "            print(f\"   [!] Connection attempt {i+1}/{max_retries} failed. Retrying in {retry_delay}s...\")\n",
    "            time.sleep(retry_delay)\n",
    "        else:\n",
    "            print(f\"   [✗] Failed to connect to Neo4j after {max_retries} attempts: {e}\")\n",
    "            print(\"   Make sure Neo4j container is running: docker-compose logs neo4j\")\n",
    "\n",
    "# Setup Local LLM\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    "    base_url=OLLAMA_URL\n",
    ")\n",
    "\n",
    "allowed_nodes = [\n",
    "    \"Person\",    # Sentient individuals like Ilea [cite: 2]\n",
    "    \"Race\",      # Biological category like Human [cite: 73]\n",
    "    \"Class\",     # System paths or academic majors [cite: 82, 149]\n",
    "    \"Skill\",     # System abilities like Identify [cite: 212]\n",
    "    \"Creature\",  # Individual monsters like the Drake [cite: 211]\n",
    "    \"Species\",   # Monster categories like Drake [cite: 209]\n",
    "    \"Location\",  # Geographical areas like the Forest [cite: 160]\n",
    "    \"Item\"       # Objects of interest like the Blue Flower [cite: 169]\n",
    "]\n",
    "\n",
    "allowed_relationships = [\n",
    "    # Identity and Biology\n",
    "    (\"Person\", \"BELONGS_TO_RACE\", \"Race\"),        # Ilea is Human [cite: 73, 116]\n",
    "    (\"Creature\", \"IS_SPECIES\", \"Species\"),        # The monster is a Drake [cite: 211]\n",
    "    \n",
    "    # System Progression\n",
    "    (\"Person\", \"HAS_CLASS\", \"Class\"),             # Ilea studies Medicine or is a Fighter [cite: 82, 149]\n",
    "    (\"Person\", \"LEARNED_SKILL\", \"Skill\"),         # Ilea learned Identify lvl 1 [cite: 212]\n",
    "    (\"Class\", \"GRANTS_SKILL\", \"Skill\"),           # A class provides specific abilities\n",
    "    \n",
    "    # World Interaction\n",
    "    (\"Person\", \"ENCOUNTERED\", \"Creature\"),        # Ilea meets the Drake [cite: 207, 210]\n",
    "    (\"Creature\", \"KILLED\", \"Creature\"),           # The Drake kills its prey [cite: 208]\n",
    "    (\"Person\", \"LOCATED_IN\", \"Location\"),         # Ilea is in the Forest [cite: 159, 160]\n",
    "    (\"Creature\", \"LOCATED_IN\", \"Location\"),       # Drakes live in the Forest [cite: 203]\n",
    "    (\"Item\", \"FOUND_IN\", \"Location\"),             # Blue flowers are in the Forest [cite: 169]\n",
    "    (\"Person\", \"HARVESTED\", \"Item\")               # Picking up special flora\n",
    "]\n",
    "\n",
    "node_properties = [\n",
    "    \"level\",             # e.g., \"lvl 1\" or \"lvl ??\" [cite: 211, 212]\n",
    "    \"description\",       # e.g., \"three meters, no wings\" [cite: 209]\n",
    "    \"rarity\",            # For items or classes\n",
    "    \"threat_rank\",       # For creatures like the Drake [cite: 211]\n",
    "    \"mana_nature\",       # For locations or items [cite: 177]\n",
    "    \"physical_appearance\" # e.g., \"glowing top\" [cite: 177]\n",
    "]\n",
    "\n",
    "# The \"Extractor\" \n",
    "# Note: LLMGraphTransformer works best with models that follow instructions well (like Llama 3.1)\n",
    "llm_transformer = LLMGraphTransformer(\n",
    "    llm=llm, \n",
    "    allowed_nodes=allowed_nodes, \n",
    "    allowed_relationships=allowed_relationships,\n",
    "    node_properties=node_properties\n",
    ")\n",
    "\n",
    "\n",
    "def save_graph_to_json(graph_documents, filename):\n",
    "    data_export = []\n",
    "    for doc in graph_documents:\n",
    "        nodes = [{\"id\": n.id, \"type\": n.type, \"properties\": n.properties} for n in doc.nodes]\n",
    "        rels = [{\n",
    "            \"source\": r.source.id,\n",
    "            \"target\": r.target.id,\n",
    "            \"type\": r.type,\n",
    "            \"properties\": r.properties\n",
    "        } for r in doc.relationships]\n",
    "        \n",
    "        data_export.append({\n",
    "            \"source_text_chunk\": doc.source.page_content,\n",
    "            \"nodes\": nodes,\n",
    "            \"relationships\": rels\n",
    "        })\n",
    "        \n",
    "    path = OUTPUT_DIR / filename\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data_export, f, indent=2)\n",
    "    print(f\"   [✓] Saved JSON to: {filename}\")\n",
    "\n",
    "def save_chunks_to_json(chunks, filename):\n",
    "    \"\"\"Save text chunks as intermediate JSON.\"\"\"\n",
    "    data_export = [\n",
    "        {\n",
    "            \"chunk_id\": i,\n",
    "            \"content\": chunk.page_content,\n",
    "            \"metadata\": chunk.metadata\n",
    "        }\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    \n",
    "    path = CHUNKS_DIR / filename\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data_export, f, indent=2)\n",
    "    print(f\"   [✓] Saved chunks to: {filename}\")\n",
    "\n",
    "def load_chunks_from_json(filename):\n",
    "    \"\"\"Load text chunks from a saved JSON file.\"\"\"\n",
    "    path = CHUNKS_DIR / filename\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Chunks file not found: {path}\")\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Reconstruct LangChain Document objects\n",
    "    from langchain_core.documents import Document\n",
    "    chunks = [\n",
    "        Document(\n",
    "            page_content=item[\"content\"],\n",
    "            metadata=item[\"metadata\"]\n",
    "        )\n",
    "        for item in data\n",
    "    ]\n",
    "    print(f\"   [✓] Loaded {len(chunks)} chunks from {filename}\")\n",
    "    return chunks\n",
    "\n",
    "def process_document(file_path, mode=\"full\"):\n",
    "    \"\"\"\n",
    "    Process a document based on the specified mode.\n",
    "    \n",
    "    Modes:\n",
    "    - \"full\": PDF → chunks → graph extraction → JSON → Neo4j\n",
    "    - \"chunks\": PDF → chunks (save JSON only)\n",
    "    - \"graph\": Load chunks JSON → graph extraction → JSON → Neo4j\n",
    "    \"\"\"\n",
    "    base_filename = os.path.basename(file_path).replace(\".pdf\", \"\")\n",
    "    chunks_filename = f\"{base_filename}_chunks.json\"\n",
    "    \n",
    "    if mode == \"graph\":\n",
    "        # Graph mode: load chunks from JSON\n",
    "        print(f\"\\nProcessing graph extraction from chunks: {base_filename}...\")\n",
    "        try:\n",
    "            chunks = load_chunks_from_json(chunks_filename)\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"   [✗] {e}\")\n",
    "            print(f\"   [!] Please run in 'chunks' or 'full' mode first to generate {chunks_filename}\")\n",
    "            return\n",
    "    else:\n",
    "        # Full/chunks mode: load PDF and extract chunks\n",
    "        print(f\"\\nProcessing: {os.path.basename(file_path)}...\")\n",
    "        \n",
    "        loader = PyPDFLoader(file_path)\n",
    "        raw_docs = loader.load()\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000, \n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(raw_docs)\n",
    "        print(f\"   [i] Split into {len(chunks)} chunks.\")\n",
    "        \n",
    "        # Save chunks to JSON\n",
    "        save_chunks_to_json(chunks, chunks_filename)\n",
    "        \n",
    "        # If mode is chunks-only, stop here\n",
    "        if mode == \"chunks\":\n",
    "            return\n",
    "    \n",
    "    print(\"   [i] Extracting graph (this may take time on CPU/iGPU)...\")\n",
    "    graph_documents = llm_transformer.convert_to_graph_documents(chunks)\n",
    "    \n",
    "    json_filename = f\"{base_filename}_graph.json\"\n",
    "    save_graph_to_json(graph_documents, json_filename)\n",
    "\n",
    "    if graph:\n",
    "        print(\"   [i] Writing to Neo4j...\")\n",
    "        graph.add_graph_documents(graph_documents)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"GraphRAG extraction pipeline\",\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        epilog=\"\"\"\n",
    "            Modes:\n",
    "            full    - Complete pipeline: PDF → chunks JSON → graph extraction → JSON → Neo4j (default)\n",
    "            chunks  - Extract text chunks only: PDF → chunks JSON (fast, for testing/reviewing)\n",
    "            graph   - Extract graph from chunks: chunks JSON → graph extraction → JSON → Neo4j (requires chunks file)\n",
    "        \"\"\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mode\",\n",
    "        choices=[\"full\", \"chunks\", \"graph\"],\n",
    "        default=\"full\",\n",
    "        help=\"Processing mode (default: full)\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # For graph-only mode, ensure Neo4j is connected\n",
    "    if args.mode in [\"full\", \"graph\"] and not graph:\n",
    "        print(\"[✗] ERROR: Could not connect to Neo4j. Cannot run in 'full' or 'graph' mode.\")\n",
    "        return\n",
    "    \n",
    "    pdf_files = glob.glob(str(INPUT_DIR / \"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(\"No PDFs found in input folder.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"[i] Running in '{args.mode}' mode\")\n",
    "    print(f\"[i] Found {len(pdf_files)} PDF(s) to process\\n\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            process_document(pdf_file, mode=args.mode)\n",
    "        except Exception as e:\n",
    "            print(f\"[✗] Error processing {pdf_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "639624fe-4239-4de6-94d0-b6b9aa8836ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [✓] Loaded 1013 chunks from /app/data/output_chunks/Azarinth_Healer_Book_5-9-794_chunks.json\n"
     ]
    }
   ],
   "source": [
    "# Get chunks\n",
    "\n",
    "chunks_filename = f\"/app/data/output_chunks/Azarinth_Healer_Book_5-9-794_chunks.json\"\n",
    "chunks = load_chunks_from_json(chunks_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e7e417a-1392-4f83-9f38-520516c298d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2026-02-01T16:52:44+00:00', 'source': '/app/data/input_docs/Azarinth_Healer_Book_5-9-794.pdf', 'total_pages': 786, 'page': 1, 'page_label': '2'}, page_content='threatening to endanger the beings of the Descent and everyone that lived in\\nHallowfort.\\nWith the immediate threats dealt with, Ilea explored the rest of the\\ndungeon accompanied by a little Fae friend she had made along the way, a\\nbeing she calls Violence. At the bottom of the facility, Ilea helped drain and\\ndisperse a large amount of gathered mana before she learned that the\\nancient facilities within the dungeon had once belonged to the Ascended,\\nbeings from another realm that had come to Elos with an unknown purpose.\\nWhen Ilea and Violence looked for answers within an enchanted device,\\nthey found out that the facility was not abandoned at all. An Ascended\\nnamed Vor Elenthir appeared and confronted Ilea, nearly killing her in the\\nensuing battle.\\nShe escaped with the help of Violence and was invited to its home,\\nwhere she learned the true nature of the Fae. With some reassurance, some\\nquestions answered, and new questions raised, she finally returned to\\nRavenhall…\\nOceanofPDF.com')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2425e05-a566-4028-8365-132372653879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34fab7fc-2681-4c8b-ad7e-c993dfe49852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_neo4j import Neo4jGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c75b2c39-8a9c-470e-919c-a232f378b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\", \"password\")\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "\n",
    "cypher_query = \"\"\"\n",
    "UNWIND $entities AS entity_name\n",
    "MATCH (center) \n",
    "WHERE toLower(center.id) CONTAINS toLower(entity_name)\n",
    "MATCH path = (center)-[rel]-(neighbor)\n",
    "RETURN center, rel, neighbor\n",
    "LIMIT 50\n",
    "\"\"\"\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=\"neo4j\",\n",
    "    password=NEO4J_PASSWORD\n",
    ")\n",
    "\n",
    "graph_data = {\"nodes\": set(), \"edges\": [], \"context_text\": []}\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\", \n",
    "    temperature=0, \n",
    "    base_url=OLLAMA_URL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c37513d6-8178-4ffd-94f0-50c3f7e88cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ilea', 'Ravenhall']\n"
     ]
    }
   ],
   "source": [
    "def extract_entities(question):\n",
    "    \"\"\"Step 2: Use LLM to extract potential entity names from the question.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"You are a Named Entity Recognition (NER) expert for a fantasy novel dataset.\n",
    "        Your task is to extract ALL meaningful entities (Persons, Locations, Monsters, Organizations, Skills) from the user's question.\n",
    "\n",
    "        Rules:\n",
    "        1. Return a comma-separated list of names.\n",
    "        2. Do not add labels like \"Entities:\" or \"Output:\".\n",
    "        3. Extract BOTH the subject (who) and the location/object (where/what).\n",
    "\n",
    "        Examples:\n",
    "        Question: \"Where is Mark?\"\n",
    "        Answer: Mark\n",
    "\n",
    "        Question: \"Did Ilea fight the Drake in the Forest?\"\n",
    "        Answer: Ilea, Drake, Forest\n",
    "\n",
    "        Question: \"How do I get to Riverwatch?\"\n",
    "        Answer: Riverwatch\n",
    "\n",
    "        Question: {question}\n",
    "        Answer:\"\"\"\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    \n",
    "    # Clean up response\n",
    "    entities = [e.strip() for e in response.content.split(',') if e.strip()]\n",
    "\n",
    "    return entities\n",
    "\n",
    "prompt = \"What was Ilea doing in Ravenhall?\"\n",
    "entities = extract_entities(prompt)\n",
    "\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ba6472e-4e56-475b-bd59-271ad84b118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = graph.query(cypher_query, {\"entities\": entities})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31557ef7-ce63-41d2-ba07-b22727a31465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'center': {'id': 'Ilea'},\n",
       "  'rel': ({'id': 'Ilea'}, 'VISITED', {'id': 'Ravenhall'}),\n",
       "  'neighbor': {'id': 'Ravenhall'}},\n",
       " {'center': {'id': 'Ilea'},\n",
       "  'rel': ({'id': 'Ilea'}, 'VISITED', {'id': 'Riverwatch'}),\n",
       "  'neighbor': {'id': 'Riverwatch'}},\n",
       " {'center': {'id': 'Ilea'},\n",
       "  'rel': ({'id': 'Ilea'}, 'ARRIVED_ON', {'id': 'Elos'}),\n",
       "  'neighbor': {'id': 'Elos'}},\n",
       " {'center': {'id': 'Ilea'},\n",
       "  'rel': ({'id': 'Ilea'}, 'COLLABORATED_WITH', {'id': 'Claire'}),\n",
       "  'neighbor': {'id': 'Claire'}},\n",
       " {'center': {'id': 'Ilea'},\n",
       "  'rel': ({'id': 'Ilea'}, 'TRAINED_WITH', {'id': 'Cerithil Hunters'}),\n",
       "  'neighbor': {'id': 'Cerithil Hunters'}},\n",
       " {'center': {'id': 'Ilea'},\n",
       "  'rel': ({'id': 'Taleen Dwarves'}, 'LEFT_MACHINES_BY', {'id': 'Ilea'}),\n",
       "  'neighbor': {'id': 'Taleen Dwarves'}},\n",
       " {'center': {'id': 'Ravenhall'},\n",
       "  'rel': ({'id': 'Ilea'}, 'VISITED', {'id': 'Ravenhall'}),\n",
       "  'neighbor': {'id': 'Ilea'}},\n",
       " {'center': {'id': 'Ravenhall'},\n",
       "  'rel': ({'id': 'Ravenhall'}, 'AFFILIATED_WITH', {'id': \"Shadow'S Hand\"}),\n",
       "  'neighbor': {'id': \"Shadow'S Hand\"}},\n",
       " {'center': {'id': 'Ravenhall'},\n",
       "  'rel': ({'id': 'Rhyvor'}, 'ADDED_RESOURCES_TO', {'id': 'Ravenhall'}),\n",
       "  'neighbor': {'id': 'Rhyvor'}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb739eb3-3df3-4847-ab5b-22de1a253ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in results:\n",
    "    source = record['center']\n",
    "    target = record['neighbor']\n",
    "    rel = record['rel']\n",
    "    \n",
    "    # Format text context for LLM\n",
    "    # \"Ilea (Person) -[ENCOUNTERED]-> Drake (Creature)\"\n",
    "    rel_type = rel[1] \n",
    "\n",
    "    # Format text context for LLM\n",
    "    context_line = f\"{source['id']} ({source.get('type','Node')}) -[{rel_type}]-> {target['id']} ({target.get('type','Node')})\"\n",
    "    graph_data[\"context_text\"].append(context_line)\n",
    "    \n",
    "    # Prepare Visual Nodes (Streamlit AGraph)\n",
    "    graph_data[\"nodes\"].add((source['id'], source.get('type', 'Unknown')))\n",
    "    graph_data[\"nodes\"].add((target['id'], target.get('type', 'Unknown')))\n",
    "    \n",
    "    # Prepare Visual Edges\n",
    "    graph_data[\"edges\"].append({\n",
    "        \"source\": source['id'],\n",
    "        \"target\": target['id'],\n",
    "        \"label\": rel_type\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fa1c64b-a004-4254-9ad3-472bb507eb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodes': {('Cerithil Hunters', 'Unknown'),\n",
       "  ('Claire', 'Unknown'),\n",
       "  ('Elos', 'Unknown'),\n",
       "  ('Ilea', 'Unknown'),\n",
       "  ('Ravenhall', 'Unknown'),\n",
       "  ('Rhyvor', 'Unknown'),\n",
       "  ('Riverwatch', 'Unknown'),\n",
       "  (\"Shadow'S Hand\", 'Unknown'),\n",
       "  ('Taleen Dwarves', 'Unknown')},\n",
       " 'edges': [{'source': 'Ilea', 'target': 'Ravenhall', 'label': 'VISITED'},\n",
       "  {'source': 'Ilea', 'target': 'Riverwatch', 'label': 'VISITED'},\n",
       "  {'source': 'Ilea', 'target': 'Elos', 'label': 'ARRIVED_ON'},\n",
       "  {'source': 'Ilea', 'target': 'Claire', 'label': 'COLLABORATED_WITH'},\n",
       "  {'source': 'Ilea', 'target': 'Cerithil Hunters', 'label': 'TRAINED_WITH'},\n",
       "  {'source': 'Ilea', 'target': 'Taleen Dwarves', 'label': 'LEFT_MACHINES_BY'},\n",
       "  {'source': 'Ravenhall', 'target': 'Ilea', 'label': 'VISITED'},\n",
       "  {'source': 'Ravenhall',\n",
       "   'target': \"Shadow'S Hand\",\n",
       "   'label': 'AFFILIATED_WITH'},\n",
       "  {'source': 'Ravenhall', 'target': 'Rhyvor', 'label': 'ADDED_RESOURCES_TO'}],\n",
       " 'context_text': ['Ilea (Node) -[VISITED]-> Ravenhall (Node)',\n",
       "  'Ilea (Node) -[VISITED]-> Riverwatch (Node)',\n",
       "  'Ilea (Node) -[ARRIVED_ON]-> Elos (Node)',\n",
       "  'Ilea (Node) -[COLLABORATED_WITH]-> Claire (Node)',\n",
       "  'Ilea (Node) -[TRAINED_WITH]-> Cerithil Hunters (Node)',\n",
       "  'Ilea (Node) -[LEFT_MACHINES_BY]-> Taleen Dwarves (Node)',\n",
       "  'Ravenhall (Node) -[VISITED]-> Ilea (Node)',\n",
       "  \"Ravenhall (Node) -[AFFILIATED_WITH]-> Shadow'S Hand (Node)\",\n",
       "  'Ravenhall (Node) -[ADDED_RESOURCES_TO]-> Rhyvor (Node)']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64924b10-8c77-4682-b643-3131791bd018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
